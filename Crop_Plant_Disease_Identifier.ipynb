{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdamJuma133/CROP_PLANT_DISEASE_IDENTIFIER-PROJECT-/blob/main/Crop_Plant_Disease_Identifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Z83tLbEFdix"
      },
      "source": [
        "# Crop Plant Disease Identifier AI Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfxyYV_VIYoR"
      },
      "source": [
        "#1. Project Structure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRzXlRvHF8Pr"
      },
      "source": [
        "This project aims to develop a multiclass crop-plant-disease-identifier using a Convolutional Neural Network (CNN) model implemented in a Google Colab notebook. The dataset contains 13 different types of crops, each with healthy and unhealthy leaf classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l96ey_khFCRG",
        "outputId": "c61d1596-8e51-44f6-9372-2b727b06a88e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aP4R2YOnIuwp"
      },
      "source": [
        "1. Exploration\n",
        "\n",
        "1.1 Design Ideation\n",
        "\n",
        "1.2 Data sourcing from Google Drive\n",
        "\n",
        "1.3 Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-vc8cgxKBkL"
      },
      "source": [
        "\n",
        "2. Model Selection\n",
        "\n",
        "2.1 Provide pre-trained models\n",
        "\n",
        "2.2 Compare performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3IBgumkKaQM"
      },
      "source": [
        "\n",
        "3. Model Training\n",
        "\n",
        "3.1 Data split\n",
        "\n",
        "3.2 Transfer learning\n",
        "\n",
        "3.3bEvaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nUtyQ9Mj2yC"
      },
      "source": [
        "# 3. Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRgTVG4gv02L"
      },
      "source": [
        "Design Ideation\n",
        "\n",
        "\n",
        "The goal of this project is to develop a multiclass crop-plant-disease identifier using a Convolutional Neural Network (CNN) model. The dataset contains 13 different types of crops, each with healthy and unhealthy leaf classes. The project will be implemented in a Google Colab notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHERpWK8wQnZ"
      },
      "source": [
        "Data Sourcing\n",
        "\n",
        "The dataset is stored in Google Drive in a folder named PROJECT_CPDI. Within this folder, there are 13 subfolders, each containing its respective crop dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d233GSFmx3xi"
      },
      "source": [
        "Data Preparation\n",
        "\n",
        "1. Connect to Google Drive:\n",
        "\n",
        "  • Mount Google Drive to access the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hP9wHeuo06bf"
      },
      "source": [
        "2. Organize Dataset:\n",
        "\n",
        "  • Ensure the dataset is properly organized into subfolders for each crop type and their respective healthy and unhealthy leaf classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUK_oDZn2a1N"
      },
      "source": [
        "3. Image Data Generators:\n",
        "\n",
        " •Use ImageDataGenerator from TensorFlow/Keras to preprocess and augment the images for training and validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRuQE-vX3j04"
      },
      "source": [
        "4. Data Splitting:\n",
        "\n",
        " •Split the data into training and validation sets using an 80-20 split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "F84CQmecNPrS"
      },
      "outputs": [],
      "source": [
        "# Exploration\n",
        "\n",
        "## Import Libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikxiT7d0NYGG",
        "outputId": "a44fdfea-574c-4de5-df45-e1a475ab72fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "## Connect to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Chj7MtPVNkgY"
      },
      "outputs": [],
      "source": [
        "# Set Dataset Path\n",
        "DATASET_PATH = '/ https://drive.google.com/drive/folders/1T4Ye1kEe93H9BXj-GpOfDoL1DEYXkluu?usp=drive_link '"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKpR9FWwNy1d",
        "outputId": "14dd26bc-7bde-4d0c-a56f-0a4b8e6b6750"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3900 images belonging to 18 classes.\n",
            "Found 967 images belonging to 18 classes.\n"
          ]
        }
      ],
      "source": [
        "# Set Dataset Path\n",
        "DATASET_PATH = '/content/drive/MyDrive/CROP_PLANT_DISEASE_IDENTIFIER(PROJECT CPDI)'  # Assuming your dataset is in a folder named PROJECT_CPDI in your Google Drive\n",
        "\n",
        "# Data Preparation\n",
        "\n",
        "# Image Data Generators\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    DATASET_PATH,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    DATASET_PATH,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "oGrC3f-PFwgx"
      },
      "outputs": [],
      "source": [
        "# Set Dataset Path\n",
        "DATASET_PATH = '/https://drive.google.com/drive/folders/1T4Ye1kEe93H9BXj-GpOfDoL1DEYXkluu ' # Removed extra forward slash from path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hy0qWQY--MNn"
      },
      "source": [
        "# Model Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thCRT3kU-ofD"
      },
      "source": [
        "Compare Performance\n",
        "\n",
        "\n",
        "We will compare the performance of VGG16 and ResNet50 models on the validation set. The comparison will be based on the validation accuracy and loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGnLW1Gl-Z77"
      },
      "source": [
        "Provide Pre-trained Models\n",
        "\n",
        "\n",
        "We will use pre-trained models VGG16 and ResNet50 for transfer learning. These models are widely used for image classification tasks and have shown good performance on various datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6d38e0e-ccc5-48f9-a714-d9b87823f5fd",
        "id": "eFFw91L-dtxJ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Found 3900 images belonging to 18 classes.\n",
            "Found 967 images belonging to 18 classes.\n",
            "Epoch 1/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1043s\u001b[0m 240s/step - accuracy: 0.0960 - loss: nan - val_accuracy: 0.0538 - val_loss: nan\n",
            "Epoch 2/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1057s\u001b[0m 246s/step - accuracy: 0.0641 - loss: nan - val_accuracy: 0.0538 - val_loss: nan\n",
            "Epoch 3/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m968s\u001b[0m 223s/step - accuracy: 0.0305 - loss: nan - val_accuracy: 0.0538 - val_loss: nan\n",
            "Epoch 4/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1023s\u001b[0m 237s/step - accuracy: 0.1018 - loss: nan - val_accuracy: 0.0538 - val_loss: nan\n",
            "Epoch 5/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m962s\u001b[0m 223s/step - accuracy: 0.0495 - loss: nan - val_accuracy: 0.0538 - val_loss: nan\n",
            "Epoch 6/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m952s\u001b[0m 220s/step - accuracy: 0.0157 - loss: nan - val_accuracy: 0.0538 - val_loss: nan\n",
            "Epoch 7/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m953s\u001b[0m 220s/step - accuracy: 0.0419 - loss: nan - val_accuracy: 0.0538 - val_loss: nan\n",
            "Epoch 8/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m953s\u001b[0m 221s/step - accuracy: 0.0422 - loss: nan - val_accuracy: 0.0538 - val_loss: nan\n",
            "Epoch 9/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m956s\u001b[0m 221s/step - accuracy: 0.0998 - loss: nan - val_accuracy: 0.0538 - val_loss: nan\n",
            "Epoch 10/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m958s\u001b[0m 222s/step - accuracy: 0.0853 - loss: nan - val_accuracy: 0.0538 - val_loss: nan\n",
            "Epoch 1/10\n",
            "\u001b[1m4/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m28s\u001b[0m 28s/step - accuracy: 0.0729 - loss: 2136.9897"
          ]
        }
      ],
      "source": [
        "# Model Selection\n",
        "\n",
        "## Import Libraries\n",
        "from tensorflow.keras.applications import VGG16, ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator #Importing ImageDataGenerator\n",
        "import os #Importing OS\n",
        "\n",
        "\n",
        "## Connect to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set Dataset Path\n",
        "DATASET_PATH = '/content/drive/MyDrive/CROP_PLANT_DISEASE_IDENTIFIER(PROJECT CPDI)'  # Assuming your dataset is in a folder named PROJECT_CPDI in your Google Drive\n",
        "\n",
        "# Data Preparation\n",
        "\n",
        "# Image Data Generators\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    DATASET_PATH,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    DATASET_PATH,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "## Load Pre-trained Models\n",
        "vgg16_base = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "resnet50_base = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "## Build Models\n",
        "def build_model(base_model):\n",
        "    x = base_model.output\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    predictions = Dense(train_generator.num_classes, activation='softmax')(x)\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    return model\n",
        "\n",
        "vgg16_model = build_model(vgg16_base)\n",
        "resnet50_model = build_model(resnet50_base)\n",
        "\n",
        "## Compile Models\n",
        "# Increased learning rate for faster training\n",
        "vgg16_model.compile(optimizer=Adam(learning_rate=0.05), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "resnet50_model.compile(optimizer=Adam(learning_rate=0.05), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "## Compare Performance\n",
        "# Reduced number of batches to 5 per epoch\n",
        "history_vgg16 = vgg16_model.fit(train_generator, validation_data=validation_generator, epochs=10, steps_per_epoch=5)\n",
        "history_resnet50 = resnet50_model.fit(train_generator, validation_data=validation_generator, epochs=10, steps_per_epoch=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XgJJXsjHaAm"
      },
      "outputs": [],
      "source": [
        "\n",
        "## Load Pre-trained Models\n",
        "vgg16_base = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "resnet50_base = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "14wzLUaNHbYt"
      },
      "outputs": [],
      "source": [
        "## Build Models\n",
        "def build_model(base_model):\n",
        "    x = base_model.output\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    predictions = Dense(train_generator.num_classes, activation='softmax')(x)\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    return model\n",
        "vgg16_model = build_model(vgg16_base)\n",
        "resnet50_model = build_model(resnet50_base)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STFdypk8HcSD"
      },
      "outputs": [],
      "source": [
        "##Compile Models\n",
        "vgg16_model.compile(optimizer=Adam(learning_rate=0.02), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "resnet50_model.compile(optimizer=Adam(learning_rate=0.02), loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIvyjSICHdGB"
      },
      "outputs": [],
      "source": [
        "# Model Selection\n",
        "\n",
        "## Import Libraries\n",
        "from tensorflow.keras.applications import VGG16, ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d7_btAFHd1w"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5QI52gFHemB"
      },
      "outputs": [],
      "source": [
        "## Load Pre-trained Models\n",
        "vgg16_base = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "resnet50_base = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cw4aTTH2Il7u"
      },
      "outputs": [],
      "source": [
        "## Build Models\n",
        "def build_model(base_model):\n",
        "    x = base_model.output\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    predictions = Dense(train_generator.num_classes, activation='softmax')(x)\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    return model\n",
        "\n",
        "vgg16_model = build_model(vgg16_base)\n",
        "resnet50_model = build_model(resnet50_base)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zrfctawAIZh"
      },
      "source": [
        "# Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 665
        },
        "id": "0_3cbgAOBK8e",
        "outputId": "8024ae7b-412a-4627-b527-26545e2ffd4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'train_generator' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-7aa39281f72c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mvgg16_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvgg16_base\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mresnet50_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresnet50_base\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-7aa39281f72c>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(base_model)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_generator' is not defined"
          ]
        }
      ],
      "source": [
        "# Model Training\n",
        "\n",
        "## Import Libraries\n",
        "from tensorflow.keras.applications import VGG16, ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "## Build Models\n",
        "def build_model(base_model):\n",
        "    x = base_model.output\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    predictions = Dense(train_generator.num_classes, activation='softmax')(x)\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    return model\n",
        "\n",
        "vgg16_model = build_model(vgg16_base)\n",
        "resnet50_model = build_model(resnet50_base)\n",
        "\n",
        "## Compile Models\n",
        "vgg16_model.compile(optimizer=Adam(learning_rate=0.0002), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "resnet50_model.compile(optimizer=Adam(learning_rate=0.0002), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "## Train Models\n",
        "history_vgg16 = vgg16_model.fit(train_generator, validation_data=validation_generator, epochs=10)\n",
        "history_resnet50 = resnet50_model.fit(train_generator, validation_data=validation_generator, epochs=10)\n",
        "\n",
        "## Evaluate the Best Model\n",
        "best_model = vgg16_model if max(history_vgg16.history['val_accuracy']) > max(history_resnet50.history['val_accuracy']) else resnet50_model\n",
        "\n",
        "# Evaluate the best model\n",
        "evaluation = best_model.evaluate(validation_generator)\n",
        "\n",
        "print(f\"Validation Loss: {evaluation[0]}\")\n",
        "print(f\"Validation Accuracy: {evaluation[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtloIAMFAOiz"
      },
      "source": [
        "Data Split\n",
        "\n",
        "The data has already been split into training and validation sets using an 80-20 split during the data preparation step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqK3Ali0Afzv"
      },
      "source": [
        "Transfer Learning\n",
        "\n",
        "We use transfer learning with pre-trained models VGG16 and ResNet50. The models are fine-tuned on our dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nAzaKNMAp3G"
      },
      "source": [
        "Evaluation\n",
        "\n",
        "We evaluate the models based on their validation accuracy and loss. The best model is selected for further use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcbG29fqJpLe"
      },
      "outputs": [],
      "source": [
        "## Load Pre-trained Models\n",
        "vgg16_base = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "resnet50_base = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training & validation accuracy and loss\n",
        "def plot_history(history, title):\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "    epochs = range(len(acc))\n",
        "\n",
        "    plt.figure(figsize=(14, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, acc, 'b', label='Training acc')\n",
        "    plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.suptitle(title)\n",
        "    plt.show()\n",
        "\n",
        "# Plot the history of VGG16\n",
        "plot_history(history_vgg16, 'VGG16 Model')\n",
        "\n",
        "# Plot the history of ResNet50\n",
        "plot_history(history_resnet50, 'ResNet50 Model')"
      ],
      "metadata": {
        "id": "_DWWnDHkudhK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}